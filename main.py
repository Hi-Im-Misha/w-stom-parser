import requests
from bs4 import BeautifulSoup
import pandas as pd


BASE_URL = "https://w-stom.ru"
CATEGORY_URL = "https://w-stom.ru/catalog/salfetki_triloks/"

import os
import requests

IMAGE_DIR = r"C:\mylife\Git_project\parser_w-stom.ru\product_images"
os.makedirs(IMAGE_DIR, exist_ok=True)

import re

def download_image(url, article, index):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∏–º–µ–Ω–µ–º."""
    try:
        # –£–±–∏—Ä–∞–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
        article = re.sub(r'[\\/:"*?<>|]', '_', article)
        
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            ext = url.split(".")[-1].split("?")[0]  # –£–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Å–ª–µ "?"
            filename = f"{article}_{index}.{ext}"
            filepath = os.path.join(IMAGE_DIR, filename)

            with open(filepath, "wb") as file:
                for chunk in response.iter_content(1024):
                    file.write(chunk)

            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")  # –ü—Ä–æ–≤–µ—Ä–∫–∞
            return filepath  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Ç—å
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url} (–∫–æ–¥ {response.status_code})")
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}")

    return ""


def parser_photo(photo_paths, url, article):

    while len(photo_paths) < 7:
        photo_paths.append("")

    return [url, article, *photo_paths]






def get_soup(url):
    headers = {"User-Agent": "Mozilla/5.0"}
    response = requests.get(url, headers=headers, stream=True)
    return BeautifulSoup(response.text, "html.parser")

def get_product_links():
    soup = get_soup(CATEGORY_URL)
    product_links = []
    for div in soup.select("div.items.productList div.item.product .productColImage a.picture"):
        link = div.get("href")
        if link:
            if link.startswith("/"):
                link = f"{BASE_URL}{link}"
            product_links.append(link)
    
    return product_links


def parse_product(url):
    soup = get_soup(url)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    breadcrumbs = [li.text.strip() for li in soup.select("li[itemprop='itemListElement'] span[itemprop='name']")][1:]
    category = breadcrumbs[0] if len(breadcrumbs) > 0 else ""
    subcategories = breadcrumbs[1:] if len(breadcrumbs) > 1 else []

    article = ""
    manufacturer = ""

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤ –∏–∑ propertyList
    property_tables = soup.select("div.propertyList div.propertyTable")
    for prop in property_tables:
        property_name = prop.select_one("div.propertyName").text.strip()
        property_value = prop.select_one("div.propertyValue").text.strip()

        if property_name == "–ê—Ä—Ç–∏–∫—É–ª":
            article = property_value  # –ê—Ä—Ç–∏–∫—É–ª
        elif property_name == "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å":
            manufacturer_link = prop.select_one("div.propertyValue a")
            manufacturer = manufacturer_link.text.strip() if manufacturer_link else property_value  # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å

    name = soup.select_one("h1.changeName")
    name = name.text.strip() if name else ""

    short_description = soup.select_one(".changeShortDescription")
    short_description = short_description.text.strip() if short_description else ""

    # –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
    full_description = get_full_description(soup)

    # –†–∞–±–æ—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    photo_tags = soup.select(".item a.zoom")
    photo_paths = []

    for i, tag in enumerate(photo_tags[:7]):
        img_url = BASE_URL + tag["href"]
        print(f"üîó –ó–∞–≥—Ä—É–∂–∞–µ–º: {img_url}")
        img_path = download_image(img_url, article, i + 1)  # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        photo_paths.append(img_path)  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É

    # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –æ—Å—Ç–∞—é—Ç—Å—è –ø—É—Å—Ç—ã–º–∏
    max_photos = 7
    while len(photo_paths) < max_photos:
        photo_paths.append("")  # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—É—Å—Ç—ã–µ –ø—É—Ç–∏

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
    price = soup.select_one(".priceVal")
    price = price.text.strip() if price else ""

    unit = soup.select_one(".measure")
    unit = unit.text.strip() if unit else ""

    print(f"–ê—Ä—Ç–∏–∫—É–ª: {article}, –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å: {manufacturer}, –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {photo_paths}")

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Å—Ç—Ä–æ–≥–æ–º –ø–æ—Ä—è–¥–∫–µ
    return [
        url, category, *subcategories[:2], article, name,
        short_description, full_description, price, unit,
        *photo_paths, manufacturer
    ]










def get_full_description(soup):
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–æ–º changeDescription
    description_blocks = soup.find_all("div", class_="changeDescription")

    for block in description_blocks:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞—Ç—Ä–∏–±—É—Ç data-first-value (–∑–Ω–∞—á–∏—Ç, —ç—Ç–æ –Ω—É–∂–Ω—ã–π –±–ª–æ–∫)
        if block.has_attr("data-first-value"):
            full_description_html = block.decode_contents()

            # –ó–∞–º–µ–Ω—è–µ–º <br> –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫, &nbsp; –Ω–∞ –ø—Ä–æ–±–µ–ª
            full_description_html = full_description_html.replace("<br>", "\n").replace("&nbsp;", " ")

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
            full_description = BeautifulSoup(full_description_html, "html.parser").get_text("\n", strip=True)
            
            return full_description  # –í—ã—Ö–æ–¥–∏–º, –∫–∞–∫ —Ç–æ–ª—å–∫–æ –Ω–∞—à–ª–∏ –Ω—É–∂–Ω—ã–π –±–ª–æ–∫

    return ""  # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É



def main():
    product_links = get_product_links()
    data = []
    for link in product_links:
        data.append(parse_product(link))
    
    columns = [
        "URL", "–ö–∞—Ç–µ–≥–æ—Ä–∏—è", "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è 1", "–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è 2", "–ê—Ä—Ç–∏–∫—É–ª", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ",
        "–û–ø–∏—Å–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–µ", "–û–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–µ", "–¶–µ–Ω–∞", "–ï–¥–∏–Ω–∏—Ü—ã (—à—Ç/—É–ø–∞–∫)",
        "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 1", "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 2", "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 3", "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 4", "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 5", "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 6", "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 7",
        "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å"
    ]

    
    df = pd.DataFrame(data, columns=columns)
    df.to_excel(r"C:\mylife\Git_project\parser_w-stom.ru\excel\products.xlsx", index=False)
    print("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ products.xlsx")

if __name__ == "__main__":
    main()
